\documentclass{article}
\usepackage{blindtext}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\newcommand{\includegraph}[1]{\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{#1}}

\title{DeepREG}
\author{Boxiang Liu}
\date{\today}
\begin{document}
\maketitle
\tableofcontents


\section{Basset} 
The Basset architecture represent the state-of-the-art for open chromatin predictions. The architecture is as follows:

\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{modeling/basset/architecture.png}

I used it for the CNN part of the network. The detailed network graph is below: 

\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/basset/model.eps}

However, the network did not train properly, likely due to too many layers.


\section{Reducing CNN layers}
Given that 3 layers won't train, I removed one layer (in directory keras1). 

\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/keras1/model.eps}

The result is quite promising. 

\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/keras1/pred_vs_obs.png}

The first layer used filter width of 100, which is quite large.


\section{Small filter}
\label{sec:small_filter}
Since most motifs are less than 20 bps, I used a filter witdh of 19 bps. 

\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/small_filter/model.eps}

The result is as good as using 100 width filters.

\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/small_filter/pred_vs_obs.png}


\section{Single layer}
When using more than one layer, either for the seq or the regulator network, the interpretability is lost. I therefore tried using one conv layer (as motif scanner) for the seq network, and no dense layer for the reg network.

\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/single_layer/model.eps}

The model worked really well. The training loss dropped to almost zero after 30 epochs, and does not show any sign of plateau (compared to \hyperref[sec:small_filter]){small filter}). However the test loss does not decrease as much indicating overfitting. 


\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/single_layer/pred_vs_obs.png}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/single_layer/loss_vs_epoch.png}

Therefore I created a new model with l1 (=1e-7) and l2 (=1e-7) on all weights regularization. Although this model prevented overfitting on the training set, the test set performance actually worsened. 

\includegraph{../figures/single_layer/20170605_092542_l1_1e-07_l2_1e-07/loss_vs_epoch.png}



\subsection{Motif discovery}
Is the network find known motifs? I took top 100 sequences with largest activation for each filter and use TomTom to match them to known motifs.

\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/dot6p_2.png}
\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/dot6p_3.png}
\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/dot6p.png}
\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/nrg1p.png}
\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/rpn4p.png}
\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/sfp1p_2.png}
\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/sfp1p.png}
\includegraphics[width=\textwidth]{../figures/single_layer/interpret/tomtom/stb3p.png}


\subsection{Maxpooling after single layer}
The best validation loss is 0.4233 which is worse than just tensor product network. 


\section{LSTM}
Using outer product network ignores the spatial information along the genome. An LSTM with temporal dimension set to the genomic coordinate should be able to incorporate additional information in theory. The network is as follows: 

\includegraph{../figures/lstm/model/model.eps}

This initial model uses an outer product as the input dimension and the genomic coordinate as the time dimension. The size of input dimension is 256*472=120832, too large to fit into memory of GeForce GTX 970. A simpler variant replaces the outer product dimension with concatenation, effective reducing the memory requirement by two orders of magnitude. 

\includegraph{../figures/lstm/concat/model.eps}

The concatenation model was trained with both SGD and ADAM. ADAM worked better than SGD. The best validation loss for LSTM is 0.3799, worse than 0.21 for tensor product network.


\section{GRU}
Since the GRU uses two gates, one gate less than LSTM, it is believed to be more computational efficient. I replaced the LSTM with GRU, and used maxpool = {15, 100, 491}. In theory, when we increase the subsample ratio to 982, the GRU should become equivalent to the concatenation network. 

\includegraph{../figures/gru/concat/model.eps}


\includegraph{../figures/gru/concat.pool.100/model.eps}


\includegraph{../figures/gru/concat.pool.491/model.eps}

Performance-wise, the best validation loss for GRU is 0.3556, worse than 0.21 for concatenation network. 


\section{Gene-Gene relationship}

\section{deepLIFT}
I rerun the \textbf{concatenation} network with \textit{valid} padding (see \textit{modeling/concatnation}) and calculated deeplift scores with respect to the regulator layer. Each of the 472 regulators receives 1056511 scores, one for each [gene,conditions] pair (there are 173 conditions $\times$ 6107 genes). I used the sums of absolute values as the overall importance score for each regulator.


\includegraph{../figures/concatenation/concat.class.deeplift/deeplift_sum_hist.pdf}

The distribution is highly skewed to the right, indicating that several important (potentially master regulators) dominates the model weights. 

The top 10 regulators are 
\begin{enumerate}
\item GAC1 / YOR178C 
\item RAS1 / YOR101W 
\item USV1 / YPL230W 
\item MSN2 / YMR037C 
\item PDR3 / YBL005W
\item YVH1 / YIR026C
\item PRR2 / YDL214C
\item SLT2 / YHR030C
\item BAS1 / YKR099W
\item SIP2 / YGL208W 
\end{enumerate}

Within these, SIP2 / YGL208W, SLT2/YHR030C, USV1 / YPL230W, GAC1 / YOR178C, MSN2 / YMR037C are also noted in Kundaje (2006) as top parents.

Since MSN2 is a known master regulator with over a hundred known targets, we tested whether MSN2 have higher deepLIFT scores for known targets. We downloaded a total of 381 genes from (http://www.yeastgenome.org/locus/S000004640/overview). Indeed, the known target has higher deepLIFT scores other genes (50.2 vs 49.9, p<0.0016). 


I also tried running deepLIFT on the regression model. When running w.r.t the regulator layer with background nucleotide frequency as the reference, the following are the top 10 genes with highest cumulative deepLIFT scores: 
# YDR277C     6.0 MTH1 / YDR277C
# YGL096W     2.0 TOS8 / YGL096W
# YGL099W     7.0 LSG1 / YGL099W
# YGR123C     3.0 PPT1 / YGR123C
# YHR136C    10.0 SPL2 / YHR136C
# YIR026C     4.0 YVH1 / YIR026C
# YKL109W     1.0 HAP4 / YKL109W
# YLR452C     9.0 SST2 / YLR452C
# YOR101W     8.0 RAS1 / YOR101W
# YPL230W     5.0 USV1 / YPL230W
Of these, MTH1, PPT1, YVH1, HAP4, USV1 are also in Kundaje 2006. 
For MSN2, I compared its score between known targets and non-targets, the p-value is 0.00046!


I tried calculating deepLIFT score for the seq layer using genomics default and gradient*input. This did not work very well. Below is an example 

genomics default:

\includegraph{../figures/concatenation/concat.regress.deeplift/deeplift_seq_0.pdf}

grad time input:

\includegraph{../figures/concatenation/concat.regress.deeplift/deeplift_seq_grad-times-inp.743129.jpg}




Later, I tried summing over the gradient*input over all 173 experiments. 


\includegraph{../figures/concatenation/concat.regress.deeplift/deeplift_seq_sum-173-exp.4281.jpg}


This seems to have worked. We can observe patches of + and - scores. However, when I summed over all experiments and genes.

\includegraph{../figures/concatenation/concat.regress.deeplift/deeplift_seq_sum-173-exp-6107-genes.pdf}

It seems the negative and positive patches are systematic! 

\section{Whole-gene and whole-experiment holdout}
They don't perform very well. The validation MSE of whole-experiment holdout is around 0.3, and that of whole-gene holdout is around 0.5. These are bad compared with 0.2 for random holdout.

\section{Improving model accuracy}
In the deep learning book, Ian give the following guidelines

First, performance metric is chosen along with the desired values. In the Street View project, he chose coverage >= 95\%. 

Second, a baseline model is established. For his project, a convolutional neural network with ReLU were used. 

Third, the baseline model is iteratively refined. One test whether each change makes an improvement. 

Fourth, compare the train and test error. If the train and test error are similar, the model is underfitting or something is wrong with the training data. Consider using a more expressive model. If that does not work, look at the worst errors because something might be wrong with the training data. If the training error is lower than the test error, that indicates overfitting and regularization can be added. 

\end{document}
